from summarizer import Summarizer

body = '''
In this and the next few videos, I want to start to talk about classification problems where the variable Y that you want to predict is discrete values will develop an algorithm called logistic regression, which is one of the most popular and most widely used learning algorithms today. Here are some examples of classification problems. Earlier we talked about email spam classification as an example of a classification problem. Another example would be classifying online transactions. So if you have a website that sells stuff and if you want to know if a particular transaction is fraudulent or not, whether someone has you know is using a stolen credit card or has stolen the user's password. There's another classification problem, and earlier we also talked about the example. Of classifying tumors as cancerous, malignant or as benign tumors. In all of these problems, the variable that we're trying to predict is a variable Y that we can think of as taking on two values, either zero or one, either spam or not spam, fraudulent or not, fraudulent malignant or benign. Another name for the class that we did note with zero is the negative class, an another name for the class that we did. Note with one is the positive class, so O may denote the benign tumor and one positive classmate denote a Malignant tumor. The assignment of the two classes spam not spam and so on. The assignment of the two clauses to positive and negative to zero, and one is somewhat arbitrary and it doesn't really matter, but often there is this intuition that the negative costs. Is conveying the absence of something like the absence of a malignant tumor, whereas one the positive causes conveying the presence of something that we may be looking for, but the declination of which is negative and which is positive is somewhat arbitrary, and it doesn't matter that much. For now, we're going to start with classification problems with just two classes, zero and one. Later on, we'll talk about multiclass problems as well, whether variable Y may take on, say, 4 value 012 and three. This is called a multi class classification problem, but for the next few videos, let's start with the two class or the binary classification problem and will worry about the multicore setting later. So how do we develop a classification algorithm? Here's an example of a training set for a classification task for classifying a tumor as malignant or benign, and notice that malignancy takes on only two values, zero or no one, or one or yes. So one thing we could do given this training set, is to apply the algorithm that we already know linear regression to this data set and just try to fit a straight line to the data. So if you take this training set. And for the straight line to it, maybe you get. I hypothesis that looks like that, right? So that's my hypothesis H of X equals Theta. Transpose X if you want to make predictions. One thing you could try doing is then threshold the classifier outputs at 0.5. That is at the vertical axis value 0.5 and if the hypothesis up was a value that is greater than, equal to oh point 5, you predict Y equals one. It was less than oh point 5. Your particular equals 0. Let's see what happens. We do that. So let's take 0.5. And so you know that's where the threshold is, and thus using linear regression. This way everything to the right of this point we would end up predicting as the positive class because the output values is greater than oh point 5 on the vertical axis and everything to the left of that point. We will end up predicting as a negative value. In this particular example, it looks like linear regression is actually doing something reasonable, even though this is a classification task we're interested in. But now let's try changing the problem a bit. Let me extend out the horizontal axis a little bit, and let's say we got one more training example way out there on the right. Notice that that additional training example this one out here. It doesn't actually change anything, right? Is looking at the training set is pretty clear whether good hypothesis is that, well, everything to the rights of somewhere around here to the right of this, we should predict this positive and everything to the left. We should probably predict is negative because from this training set it looks like all the tumors larger than a certain value around here on malignant an all the tumors smaller than that are not malignant, at least for this training set. But once without it, that extra example out here. If you now run linear regression, you instead get a straight line fit to the data. That might maybe look like this. And if you know threshold this hypothesis at Oh Point 5, you end up with a threshold that's around here so that everything to the right of this point you predict this positive and everything to the left of that point. You predict this negative. And this seems a pretty bad thing for linear regression to have done right. Because you know these are a positive examples. These are a negative examples is pretty clear which really should be. Separating the two classes somewhere around there, but somehow by adding one example way out here to the right, this example really isn't giving us any new information. I mean, it should be no surprise to the learning algorithm that example way out here. It turns out to be malignant, but somehow adding that example of there cause linear regression. The change is straight line fit to the data from this magenta from this magenta line out here to this blue line over here and caused it to give us a worse hypothesis. So applying linear regression to a classification problem usually is into often isn't a great idea in the first instance. In the first example, before I added this extra extra training example, previously linear regression was just getting lucky and it got us a hypothesis that you know works well for that particular example. But usually applying linear regression to a data set you know you might get lucky, but often it isn't a good idea, so I wouldn't use linear regression. For classification problems, here's one. Other funny thing about what would happen if we were to use linear regression for classification problem. For classification, we know that why is either zero or one. But if you are using linear regression, while the hypothesis can output values that much larger than one or less than zero, even if all of your training examples have labels Y equals 0 or one. And it seems kind of strange that even though we know that the labels should be 01, it seems kind of strange if the algorithm can output values much larger than one or much smaller than 0. So what we'll do in the next few videos is develop an algorithm called logistic regression, which has the property that the output, the predictions of logistic regression are always between zero and one, and doesn't become bigger than one. Or become listen O. And by the way, the Jessica regression is and we will use it as a classification algorithm. Is some, maybe sometimes confusing that the term regression appears in his name even though logistic regression is actually a classification algorithm. But that's just the name it was given for historical reasons, so don't be confused by that. Logistic regression is actually a classification algorithm that we apply to settings where the label Y is the speed value when is either 01. So hopefully you now know why. If you have a classification problem, using linear regression isn't a good idea. In the next video, we'll start working out the details of the logistic regression algorithm. 
'''
raw_list = body.split('.')

model = Summarizer()
result = model(body, min_length=60)
brief = ''.join(result)
print(brief)
brief_list = brief.split('.')
"""
The Chrysler Building, the famous art deco New York skyscraper, will be sold for a small fraction of its previous sales price. 
The building sold fairly quickly after being publicly placed on the market only two months ago.
The incentive to sell the building at such a huge loss was due to the soaring rent the owners pay to Cooper Union, a New York college, for the land under the building.'
Still the building is among the best known in the city, even to people who have never been to New York.
"""

print('original length: ', len(body))
print('summary length: ', len(brief))
print('original/summary: ', len(body)/len(brief))
print('original sentence count: ', len(raw_list))
print('summary sentence count: ', len(brief_list))
